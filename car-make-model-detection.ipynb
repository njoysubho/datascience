{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":46697,"sourceType":"datasetVersion","datasetId":31559}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nimport torch.optim as optim\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T06:29:55.688013Z","iopub.execute_input":"2025-06-26T06:29:55.688303Z","iopub.status.idle":"2025-06-26T06:29:55.693026Z","shell.execute_reply.started":"2025-06-26T06:29:55.688284Z","shell.execute_reply":"2025-06-26T06:29:55.692132Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"IMG_DIR = '/kaggle/input/stanford-car-dataset-by-classes-folder'\nCSV_PATH = '/kaggle/input/stanford-car-dataset-by-classes-folder/anno_test.csv'\nBATCH_SIZE = 32\nNUM_WORKERS = 4\nNUM_EPOCHS = 10\nLR = 1e-4\nCROP_IMAGES = True\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T06:29:07.393176Z","iopub.execute_input":"2025-06-26T06:29:07.393488Z","iopub.status.idle":"2025-06-26T06:29:07.404262Z","shell.execute_reply.started":"2025-06-26T06:29:07.393458Z","shell.execute_reply":"2025-06-26T06:29:07.403572Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"column_names = [\"image_name\", \"x1\", \"y1\", \"x2\", \"y2\", \"class_id\"]\ndf = pd.read_csv(CSV_PATH,names=column_names)\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T06:29:07.405410Z","iopub.execute_input":"2025-06-26T06:29:07.405613Z","iopub.status.idle":"2025-06-26T06:29:07.430360Z","shell.execute_reply.started":"2025-06-26T06:29:07.405596Z","shell.execute_reply":"2025-06-26T06:29:07.429780Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"     image_name   x1   y1    x2   y2  class_id\n0     00001.jpg   30   52   246  147       181\n1     00002.jpg  100   19   576  203       103\n2     00003.jpg   51  105   968  659       145\n3     00004.jpg   67   84   581  407       187\n4     00005.jpg  140  151   593  339       185\n...         ...  ...  ...   ...  ...       ...\n8036  08037.jpg   49   57  1169  669        63\n8037  08038.jpg   23   18   640  459        16\n8038  08039.jpg   33   27   602  252        17\n8039  08040.jpg   33  142   521  376        38\n8040  08041.jpg   77   73   506  380        32\n\n[8041 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>x1</th>\n      <th>y1</th>\n      <th>x2</th>\n      <th>y2</th>\n      <th>class_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00001.jpg</td>\n      <td>30</td>\n      <td>52</td>\n      <td>246</td>\n      <td>147</td>\n      <td>181</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00002.jpg</td>\n      <td>100</td>\n      <td>19</td>\n      <td>576</td>\n      <td>203</td>\n      <td>103</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00003.jpg</td>\n      <td>51</td>\n      <td>105</td>\n      <td>968</td>\n      <td>659</td>\n      <td>145</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00004.jpg</td>\n      <td>67</td>\n      <td>84</td>\n      <td>581</td>\n      <td>407</td>\n      <td>187</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00005.jpg</td>\n      <td>140</td>\n      <td>151</td>\n      <td>593</td>\n      <td>339</td>\n      <td>185</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8036</th>\n      <td>08037.jpg</td>\n      <td>49</td>\n      <td>57</td>\n      <td>1169</td>\n      <td>669</td>\n      <td>63</td>\n    </tr>\n    <tr>\n      <th>8037</th>\n      <td>08038.jpg</td>\n      <td>23</td>\n      <td>18</td>\n      <td>640</td>\n      <td>459</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>8038</th>\n      <td>08039.jpg</td>\n      <td>33</td>\n      <td>27</td>\n      <td>602</td>\n      <td>252</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>8039</th>\n      <td>08040.jpg</td>\n      <td>33</td>\n      <td>142</td>\n      <td>521</td>\n      <td>376</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>8040</th>\n      <td>08041.jpg</td>\n      <td>77</td>\n      <td>73</td>\n      <td>506</td>\n      <td>380</td>\n      <td>32</td>\n    </tr>\n  </tbody>\n</table>\n<p>8041 rows Ã— 6 columns</p>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"class CarsDataset(Dataset):\n    def __init__(self, file_list, annotations_df, class_to_idx, transform=None, crop=True):\n        self.samples = file_list\n        self.annotations = annotations_df\n        self.transform = transform\n        self.crop = crop\n        self.class_to_idx = class_to_idx\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        img_path, label_name = self.samples[idx]\n        label = self.class_to_idx[label_name]\n        img_name = os.path.basename(img_path)\n        \n        image = Image.open(img_path).convert(\"RGB\")\n        if self.crop:\n            bbox = self.annotations[self.annotations['image'] == img_name]\n            if not bbox.empty:\n                x1, y1, x2, y2 = bbox.iloc[0][['x1', 'y1', 'x2', 'y2']]\n                image = image.crop((x1, y1, x2, y2))\n        \n        if self.transform:\n            image = self.transform(image)\n\n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T06:29:07.431065Z","iopub.execute_input":"2025-06-26T06:29:07.431302Z","iopub.status.idle":"2025-06-26T06:29:07.437345Z","shell.execute_reply.started":"2025-06-26T06:29:07.431286Z","shell.execute_reply":"2025-06-26T06:29:07.436614Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"column_names = [\"image\", \"x1\", \"y1\", \"x2\", \"y2\", \"class_id\"]\nannotations_df = pd.read_csv('/kaggle/input/stanford-car-dataset-by-classes-folder/anno_train.csv', names=column_names)\nsamples = []\nfor root, _, files in os.walk(IMG_DIR):\n    for file in files:\n        if file.endswith('.jpg'):\n            class_name = os.path.basename(root)\n            samples.append((os.path.join(root, file), class_name))\nclass_names = sorted(set(cls for _, cls in samples))\nclass_to_idx = {cls: i for i, cls in enumerate(class_names)}\n\ntrain_files, val_files = train_test_split(samples, test_size=0.2, random_state=42, stratify=[c for _, c in samples])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T06:29:07.438671Z","iopub.execute_input":"2025-06-26T06:29:07.438862Z","iopub.status.idle":"2025-06-26T06:29:12.519249Z","shell.execute_reply.started":"2025-06-26T06:29:07.438833Z","shell.execute_reply":"2025-06-26T06:29:12.518656Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"num_classes = df['class_id'].unique()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T06:29:12.519848Z","iopub.execute_input":"2025-06-26T06:29:12.520033Z","iopub.status.idle":"2025-06-26T06:29:12.523899Z","shell.execute_reply.started":"2025-06-26T06:29:12.520019Z","shell.execute_reply":"2025-06-26T06:29:12.523033Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"\ntrain_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n   # transforms.RandomResizedCrop(224),\n    #transforms.RandomHorizontalFlip(),\n    #transforms.ColorJitter(0.2, 0.2, 0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T06:32:31.277217Z","iopub.execute_input":"2025-06-26T06:32:31.277508Z","iopub.status.idle":"2025-06-26T06:32:31.283062Z","shell.execute_reply.started":"2025-06-26T06:32:31.277484Z","shell.execute_reply":"2025-06-26T06:32:31.282303Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"train_dataset = CarsDataset(train_files, annotations_df, class_to_idx,transform=train_transforms, crop=False)\nval_dataset = CarsDataset(val_files, annotations_df, class_to_idx,transform=val_transforms, crop=False)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T06:32:32.795885Z","iopub.execute_input":"2025-06-26T06:32:32.796194Z","iopub.status.idle":"2025-06-26T06:32:32.801001Z","shell.execute_reply.started":"2025-06-26T06:32:32.796174Z","shell.execute_reply":"2025-06-26T06:32:32.800239Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load ResNet18 and modify the final layer\nmodel = models.resnet50(pretrained=True)\n\nfor param in model.parameters():\n    param.requires_grad = False\n\nmodel.fc = nn.Linear(model.fc.in_features, len(num_classes))\n\nfor param in model.fc.parameters():\n    param.requires_grad = True\n\nmodel = model.to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, \n                       steps_per_epoch=len(train_loader), epochs=5)\n\nnum_epochs = 5\nbest_val_acc = 0.0\n\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n    model.train()\n    running_loss = 0.0\n    train_correct = 0\n    train_total = 0\n\n    for inputs, labels in tqdm(train_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        _, predicted = outputs.max(1)\n        train_total += labels.size(0)\n        train_correct += predicted.eq(labels).sum().item()\n\n    avg_train_loss = running_loss / train_total\n    train_acc = train_correct / train_total\n    print(f\"Training Loss: {avg_train_loss:.4f}, Accuracy: {train_acc * 100:.2f}%\")\n\n    # Validation\n    model.eval()\n    correct = 0\n    total = 0\n    val_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * inputs.size(0)\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n    avg_val_loss = val_loss / total\n    val_acc = correct / total\n    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_acc * 100:.2f}%\")\n\n    # Step scheduler if it's per-epoch\n    scheduler.step()\n\n    # Save best model\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), \"best_resnet50.pth\")\n        print(\"âœ… Best model saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T06:32:35.359581Z","iopub.execute_input":"2025-06-26T06:32:35.360193Z","iopub.status.idle":"2025-06-26T06:38:26.416550Z","shell.execute_reply.started":"2025-06-26T06:32:35.360170Z","shell.execute_reply":"2025-06-26T06:38:26.415475Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 405/405 [00:55<00:00,  7.30it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 5.4939, Accuracy: 16.03%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 4.1671, Accuracy: 24.53%\nâœ… Best model saved.\n\nEpoch 2/5\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 405/405 [00:55<00:00,  7.33it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 3.4226, Accuracy: 36.44%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 3.8737, Accuracy: 33.12%\nâœ… Best model saved.\n\nEpoch 3/5\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 405/405 [00:55<00:00,  7.31it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 2.7861, Accuracy: 46.09%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 4.1187, Accuracy: 33.70%\nâœ… Best model saved.\n\nEpoch 4/5\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 405/405 [00:55<00:00,  7.30it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 2.3832, Accuracy: 52.22%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 3.8239, Accuracy: 37.63%\nâœ… Best model saved.\n\nEpoch 5/5\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 405/405 [00:55<00:00,  7.32it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss: 2.2557, Accuracy: 54.80%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 3.9163, Accuracy: 37.69%\nâœ… Best model saved.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torchvision\n\n# Get a batch of images and labels from the train_loader\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n\n# Make a grid of 10 images (un-normalized for display)\nimg_grid = torchvision.utils.make_grid(images[:10], nrow=5)\n\n# Unnormalize (if you used standard ImageNet normalization)\nmean = torch.tensor([0.485, 0.456, 0.406]).reshape(3,1,1)\nstd = torch.tensor([0.229, 0.224, 0.225]).reshape(3,1,1)\nimg_grid = img_grid * std + mean  # unnormalize\n\n# Convert to numpy and plot\nnpimg = img_grid.permute(1, 2, 0).numpy()\nplt.figure(figsize=(10, 5))\nplt.imshow(npimg)\nplt.axis('off')\nplt.title(\"Sample Images from train_loader\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T06:29:13.072928Z","iopub.status.idle":"2025-06-26T06:29:13.073197Z","shell.execute_reply.started":"2025-06-26T06:29:13.073073Z","shell.execute_reply":"2025-06-26T06:29:13.073085Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}