{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.nb_01 import *\n",
    "from fastai import datasets\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_data():\n",
    "    path=datasets.download_data(MNIST_URL,ext='.gz')\n",
    "    with gzip.open(path) as f:\n",
    "        ((x_train,y_train),(x_valid,y_valid),_)=pickle.load(f,encoding='latin-1')\n",
    "    return map(tensor,(x_train,y_train,x_valid,y_valid))\n",
    "\n",
    "def normalize(x,m,s):return (x-m)/s\n",
    "def near(a,b): return torch.allclose(a, b, rtol=1e-3, atol=1e-5)\n",
    "def test_near(a,b): test(a,b,near)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(x):\n",
    "    return x.mean(),x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_valid,y_valid=get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean=x_train.mean()\n",
    "train_std=x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.4239), tensor(-0.4437))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step1 We normalize our input. We normalize input because actual input values can contain very high numbers\n",
    "#repeated multiplication can explode those numbers and hence it will be very hard for SGD to converge. \n",
    "# Rember when we normalize we normalize both the train data and valid data with same mean and standard variation.\n",
    "# Otherwise they will be statistically different.\n",
    "x_train=normalize(x_train,train_mean,train_std)\n",
    "x_valid=normalize(x_valid,train_mean,train_std)\n",
    "x_train.mean(),x_valid.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, tensor(10), 50)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,m=x_train.shape\n",
    "c=y_train.max()+1\n",
    "nh=50\n",
    "n,m,c,nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,n_in,n_h,n_out):\n",
    "        super().__init__()\n",
    "        self.layers=[nn.Linear(n_in,n_h),nn.ReLU(),nn.Linear(n_h,n_out)]\n",
    "    def __call__(self,x):\n",
    "        for l in self.layers:x=l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(m,nh,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1365,  0.1784,  0.3573,  ...,  0.4475,  0.1477,  0.2556],\n",
       "        [ 0.9111,  0.1356,  0.8911,  ...,  0.1095,  0.2561,  0.5328],\n",
       "        [-0.1127,  0.5313,  1.5855,  ...,  0.4406,  2.2535, -0.3431],\n",
       "        ...,\n",
       "        [ 1.2885,  1.0124,  1.3355,  ..., -0.8332,  1.0287, -0.3092],\n",
       "        [ 1.9323,  0.7244,  1.2166,  ...,  0.1089,  0.7854,  0.7096],\n",
       "        [ 0.7867,  0.6502,  0.6499,  ..., -0.4622,  0.6458,  0.1509]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We just ran once and get pred\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets use Cross Entropy as loss function\n",
    "#Cross entropy calculated as \n",
    "def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_pred=log_softmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.1463, -1.9979, -3.1365], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[[0,1,2], [5,0,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target): return -input[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nll(sm_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4100, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can rewrite log_softmax function by using log rules\n",
    "def log_softmax(x): return x - (x.exp().sum(-1,keepdim=True)).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(nll(log_softmax(pred),y_train),loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(F.nll_loss(F.log_softmax(pred,-1),y_train),loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what we achieved till now \n",
    "### We created our own model and we run a single forward pass\n",
    "### Instead of using MSE as loss function we use log_softmax, we add negative log loss on that softmax\n",
    "### We use nll_lose and log_softmax from pytorch and prove both result are actually same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch training Model and BackPropagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func=F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 784]), torch.Size([64]), torch.Size([64, 10]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we will train our model based on batch\n",
    "bs=64\n",
    "xb=x_train[0:bs]\n",
    "yb=y_train[0:bs]\n",
    "preds=model(xb)\n",
    "xb.shape,yb.shape,preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4785, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(preds,yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1094)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds,yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.1\n",
    "epoch=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epoch):\n",
    "    for i in range((n-1)//bs+1):\n",
    "        start_i=i*bs\n",
    "        end_i=start_i+bs\n",
    "        xb=x_train[start_i:end_i]\n",
    "        yb=y_train[start_i:end_i]\n",
    "        loss=loss_func(model(xb),yb)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l,'weight'):\n",
    "                    l.weight-=l.weight.grad*lr\n",
    "                    l.bias-=l.bias.grad*lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0020, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb),yb), accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epoch):\n",
    "    for i in range((n-1)//bs+1):\n",
    "        start_i=i*bs\n",
    "        end_i=start_i+bs\n",
    "        xb=x_train[start_i:end_i]\n",
    "        yb=y_train[start_i:end_i]\n",
    "        loss=loss_func(model(xb),yb)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for p in model.parameters():\n",
    "                p-=p.grad*lr\n",
    "                model.zero_grad()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0020, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb),yb), accuracy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Till now our model has hard coded layers , what if we can pass the layers are argument when we create the module. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self,layers):\n",
    "        super().__init__()\n",
    "        self.layers=layers\n",
    "        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}',l)\n",
    "    def __call__(self,x):\n",
    "        for l in self.layers: x=l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[nn.Linear(m,nh),nn.ReLU(),nn.Linear(nh,10)]\n",
    "model=Model(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can refactor better using modulelist\n",
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self,layers):\n",
    "        super().__init__()\n",
    "        self.layers=nn.ModuleList(layers)\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        for l in self.layers: x=l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our hand made class can be replaced with\n",
    "nn.Sequential??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr\n",
    "        \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params: p -= p.grad * self.lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params: p.grad.data.zero_()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=nn.Sequential(nn.Linear(m,nh),nn.ReLU(),nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=Optimizer(model.parameters(),0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Optimizer at 0x7ff407841fd0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epoch):\n",
    "    for i in range((n-1)//bs+1):\n",
    "        start_i=i*bs\n",
    "        end_i=start_i+bs\n",
    "        xb=x_train[start_i:end_i]\n",
    "        yb=y_train[start_i:end_i]\n",
    "        loss=loss_func(model(xb),yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.3232, grad_fn=<NllLossBackward>), tensor(0.))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.SGD??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_model():\n",
    "    model=nn.Sequential(nn.Linear(m,nh),nn.ReLU(),nn.Linear(nh,10))\n",
    "    opt=optim.SGD(model.parameters(),lr=0.1)\n",
    "    return model,opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt=get_model()\n",
    "for e in range(epoch):\n",
    "    for i in range((n-1)//bs+1):\n",
    "        start_i=i*bs\n",
    "        end_i=start_i+bs\n",
    "        xb=x_train[start_i:end_i]\n",
    "        yb=y_train[start_i:end_i]\n",
    "        loss=loss_func(model(xb),yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0006, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. run forward pass, 2. run loss function, 3. loss.backward(), 4. upgrade model grad, 5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Dataset():\n",
    "    def __init__(self,x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "    def __getitem__(self,i):\n",
    "        return self.x[i],self.y[i]\n",
    "    def __len__(self): return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
    "assert len(train_ds)==len(x_train)\n",
    "assert len(valid_ds)==len(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt=get_model()\n",
    "for e in range(epoch):\n",
    "    for i in range((n-1)//bs+1):\n",
    "        start_i=i*bs\n",
    "        end_i=start_i+bs\n",
    "        xb,yb=train_ds[start_i:end_i]\n",
    "        loss=loss_func(model(xb),yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0032, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, bs): self.ds,self.bs = ds,bs\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt=get_model()\n",
    "trains_dl=DataLoader(train_ds,bs=64)\n",
    "for e in range(epoch):\n",
    "    for xb,yb in trains_dl:\n",
    "        predict=model(xb)\n",
    "        loss=loss_func(predict,yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0020, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for e in range(epoch):\n",
    "        for xb,yb in trains_dl:\n",
    "            predict=model(xb)\n",
    "            loss=loss_func(predict,yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0003, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add sampling capability\n",
    "class Sampler():\n",
    "    def __init__(self,ds,bs,shuffle=True):\n",
    "        self.n,self.bs,self.shuffle=len(ds),bs,shuffle\n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    xs,ys = zip(*b)\n",
    "    return torch.stack(xs),torch.stack(ys)\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl=DataLoader(train_ds,Sampler(train_ds,bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb,yb=next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now time to use pytorch data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl=DataLoader(train_ds,bs,sampler=RandomSampler(train_ds),collate_fn=collate)\n",
    "valid_dl=DataLoader(valid_ds,bs,sampler=SequentialSampler(valid_ds),collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb,yb=next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2056, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for xb,yb in train_dl:\n",
    "            pred=model(xb)\n",
    "            loss=loss_func(pred,yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc = 0.,0.\n",
    "            for xb,yb in valid_dl:\n",
    "                pred=model(xb)\n",
    "                tot_loss+=loss_func(pred,yb)\n",
    "                tot_acc+=accuracy(pred,yb)\n",
    "        nv=len(valid_dl)\n",
    "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
    "    return tot_loss/nv, tot_acc/nv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.2398) tensor(0.9305)\n",
      "1 tensor(0.1351) tensor(0.9602)\n",
      "2 tensor(0.1765) tensor(0.9513)\n",
      "3 tensor(0.1665) tensor(0.9614)\n",
      "4 tensor(0.1269) tensor(0.9667)\n",
      "5 tensor(0.2314) tensor(0.9430)\n",
      "6 tensor(0.1445) tensor(0.9650)\n",
      "7 tensor(0.3164) tensor(0.9296)\n",
      "8 tensor(0.1882) tensor(0.9584)\n",
      "9 tensor(0.1410) tensor(0.9658)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.1410), tensor(0.9658))"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt=get_model()\n",
    "fit(10,model,F.cross_entropy,opt,train_dl,valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_dls(train_ds,valid_ds,bs,**kwargs):\n",
    "    return DataLoader(train_ds,bs,shuffle=True,**kwargs),DataLoader(valid_ds,bs,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.2398) tensor(0.9239)\n",
      "1 tensor(0.2454) tensor(0.9328)\n",
      "2 tensor(0.4536) tensor(0.8695)\n",
      "3 tensor(0.1328) tensor(0.9640)\n",
      "4 tensor(0.1271) tensor(0.9678)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.1271), tensor(0.9678))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl,valid_dl=get_dls(train_ds,valid_ds,bs)\n",
    "model,opt=get_model()\n",
    "fit(5,model,F.cross_entropy,opt,train_dl,valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 03_minibatch_training.ipynb to exp/nb_03.py\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 03_minibatch_training.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
