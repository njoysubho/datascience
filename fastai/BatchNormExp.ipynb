{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.nb_03 import *\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_valid,y_valid=get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let say I don't normalize my input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds,valid_ds=Dataset(x_train,y_train),Dataset(x_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl,valid_dl=get_dls(train_ds,valid_ds,bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, tensor(10), 50)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nh=50\n",
    "c=y_train.max()+1\n",
    "ni=x_train.shape[1]\n",
    "ni,c,nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self,ni,nh,c):\n",
    "        super().__init__()\n",
    "        self.ni=ni\n",
    "        self.nh=nh\n",
    "        self.c=c\n",
    "        self.layers=nn.ModuleList([nn.Linear(ni,nh),nn.BatchNorm1d(nh),nn.ReLU(),nn.BatchNorm1d(nh),nn.Linear(nh,10)])\n",
    "    def forward(self,x):\n",
    "        for l in self.layers:\n",
    "            x=l(x)\n",
    "           ## print(f\"After layer => {l},mean=>{x.mean()},std=>{x.std()}\")\n",
    "        return x\n",
    "\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MyModel(ni,nh,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=optim.SGD(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at0 ==> 2.0353126525878906\n",
      "Loss at1 ==> 10.691108703613281\n",
      "Loss at2 ==> 42.754005432128906\n",
      "Loss at3 ==> 394.425537109375\n",
      "Loss at4 ==> 813.5791625976562\n",
      "Loss at5 ==> 33276.375\n",
      "Loss at6 ==> 40714.3203125\n",
      "Loss at7 ==> 495217.75\n",
      "Loss at8 ==> 1725694.75\n",
      "Loss at9 ==> 33495224.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for xb,yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = F.cross_entropy(pred,yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad\n",
    "    print(f\"Loss at{epoch} ==> {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BatchNorm1d??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#say now we normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_valid,y_valid=get_data()\n",
    "mean=x_train.mean()\n",
    "std=x_train.std()\n",
    "x_train=normalize(x_train,mean,std)\n",
    "x_valid=normalize(x_valid,mean,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds,valid_ds=Dataset(x_train,y_train),Dataset(x_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl,valid_dl=get_dls(train_ds,valid_ds,bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self,ni,nh,c):\n",
    "        super().__init__()\n",
    "        self.ni=ni\n",
    "        self.nh=nh\n",
    "        self.c=c\n",
    "        self.layers=nn.ModuleList([nn.Linear(ni,nh),nn.BatchNorm1d(nh),nn.ReLU(),nn.Linear(nh,10)])\n",
    "    def forward(self,x):\n",
    "        for l in self.layers:\n",
    "            x=l(x)\n",
    "            print(f\"After layer => {l},mean=>{x.mean()},std=>{x.std()}\")\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=nn.Sequential(\n",
    "    nn.Linear(ni,nh),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(nh),\n",
    "    nn.Linear(nh,10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for xb,yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = F.cross_entropy(pred,yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb=next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=F.cross_entropy(model(xb),yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3446, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0126), tensor(1.0155))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.mean(),xb.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import data_block\n",
    "from fastai import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self,nf,mom=0.9,eps=0.01):\n",
    "        super().__init__()\n",
    "        self.nf=nf\n",
    "        self.mom=mom\n",
    "        self.eps=eps\n",
    "        self.mults=nn.Parameter(torch.ones(nf,1,1))\n",
    "        self.adds=nn.Parameter(torch.zeros(nf,1,1))\n",
    "        self.register_buffer('vars',torch.ones(1,nf,1,1))\n",
    "        self.register_buffer('means',torch.zeroes(1,nf,1,1))\n",
    "    def update_stats(x):\n",
    "        m=x.mean((0,2,3),keepdim=True)\n",
    "        v=x.var((0,2,3),keepdim=True)\n",
    "        self.means.lerp_(m,self.mom)\n",
    "        self.means.lerp_(v,self.mom)\n",
    "    def forward(self,x):\n",
    "        if self.training:\n",
    "            with torch.no_grad():      \n",
    "                m,v=self.update_stats(x)\n",
    "        else:\n",
    "            m,v=self.means,self.vars\n",
    "        x=(x-m)/(v+self.eps).sqrt()\n",
    "        return self.mults*x+self.adds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
