{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-10T05:34:03.644702Z","iopub.execute_input":"2023-07-10T05:34:03.645854Z","iopub.status.idle":"2023-07-10T05:34:03.679062Z","shell.execute_reply.started":"2023-07-10T05:34:03.645808Z","shell.execute_reply":"2023-07-10T05:34:03.677912Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"X= np.random.rand(4,50)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T05:34:03.681162Z","iopub.execute_input":"2023-07-10T05:34:03.681646Z","iopub.status.idle":"2023-07-10T05:34:03.687738Z","shell.execute_reply.started":"2023-07-10T05:34:03.681589Z","shell.execute_reply":"2023-07-10T05:34:03.686873Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"Y = np.random.choice([0,1],size=50*1)\nY=Y.reshape((1,50))","metadata":{"execution":{"iopub.status.busy":"2023-07-10T05:34:03.689133Z","iopub.execute_input":"2023-07-10T05:34:03.690217Z","iopub.status.idle":"2023-07-10T05:34:03.707150Z","shell.execute_reply.started":"2023-07-10T05:34:03.690158Z","shell.execute_reply":"2023-07-10T05:34:03.705718Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"We assume a 3 layer neural network with [4,4,1] nodes in layer respectively. ","metadata":{}},{"cell_type":"code","source":"def sigmoid(x):\n    return 1/(1+np.exp(-x))","metadata":{"execution":{"iopub.status.busy":"2023-07-10T05:34:03.710367Z","iopub.execute_input":"2023-07-10T05:34:03.710895Z","iopub.status.idle":"2023-07-10T05:34:03.718933Z","shell.execute_reply.started":"2023-07-10T05:34:03.710851Z","shell.execute_reply":"2023-07-10T05:34:03.717819Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    return np.exp(x) / np.sum(np.exp(x), axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T05:34:03.720222Z","iopub.execute_input":"2023-07-10T05:34:03.720780Z","iopub.status.idle":"2023-07-10T05:34:03.732129Z","shell.execute_reply.started":"2023-07-10T05:34:03.720748Z","shell.execute_reply":"2023-07-10T05:34:03.731294Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def relu(x):\n    return np.maximum(0,x)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T05:34:03.733488Z","iopub.execute_input":"2023-07-10T05:34:03.733840Z","iopub.status.idle":"2023-07-10T05:34:03.744380Z","shell.execute_reply.started":"2023-07-10T05:34:03.733810Z","shell.execute_reply":"2023-07-10T05:34:03.743447Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"In the below I am initializing Ws and bs. Most confusing part here is dimension , I am following Andrew Ng's approach \nX= [numOfFeature,numOfSamples] -> so if we have 4 features and we have 50 samples then X is of shape (4,50)\nW = [numOfNodesInCurrentLayer, numNodesPreviousLayer] ->  for W1 which is the first layer previous layer is input or X  ","metadata":{}},{"cell_type":"code","source":"numOfFeatures = X.shape[0]\nnumOfSamples = X.shape[1]\nW1 = np.random.randn(4,numOfFeatures)\nb1 = np.zeros((4,1))\nW2 = np.random.randn(4,4)\nb2 = np.zeros((4,1))\nW3 = np.random.randn(1,4)\nb3 = np.zeros((1,1))\nZ1 = np.dot(W1,X)+b1\nA1 = relu(Z1)\nZ2 = np.dot(W2,A1)+b2\nA2 = relu(Z2)\nZ3 = np.dot(W3,A2)+b3\nA3 = sigmoid(Z3)\nA3.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-10T05:34:05.034664Z","iopub.execute_input":"2023-07-10T05:34:05.035307Z","iopub.status.idle":"2023-07-10T05:34:05.051055Z","shell.execute_reply.started":"2023-07-10T05:34:05.035262Z","shell.execute_reply":"2023-07-10T05:34:05.050136Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(1, 50)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see how different our predicted output is from actual output. Our objective is to become as close as possible to actual output.\nObserving the below values show us that we are way off. \nWe need some measurement of how bad our prediction is , this is call a loss function. \nloss function I chose here is negative log loss. \nLoss = -1/m\\sum(ylog(yhat)+(1-y)log(1-yhat))\nlook at the minus sign in front. In order to minimise the loss we need to maximise the function","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"Y.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-10T05:34:12.927767Z","iopub.execute_input":"2023-07-10T05:34:12.928310Z","iopub.status.idle":"2023-07-10T05:34:12.938323Z","shell.execute_reply.started":"2023-07-10T05:34:12.928263Z","shell.execute_reply":"2023-07-10T05:34:12.936698Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(1, 50)"},"metadata":{}}]},{"cell_type":"code","source":"A3","metadata":{"execution":{"iopub.status.busy":"2023-07-10T05:34:14.445907Z","iopub.execute_input":"2023-07-10T05:34:14.446397Z","iopub.status.idle":"2023-07-10T05:34:14.455500Z","shell.execute_reply.started":"2023-07-10T05:34:14.446362Z","shell.execute_reply":"2023-07-10T05:34:14.454327Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"array([[0.64623702, 0.72193132, 0.89548126, 0.75797992, 0.68418538,\n        0.8077303 , 0.93802796, 0.88917153, 0.72176088, 0.76774041,\n        0.55118573, 0.88814537, 0.83400178, 0.83554479, 0.85373738,\n        0.95440749, 0.88896933, 0.7146725 , 0.78574545, 0.73689759,\n        0.96383887, 0.83919149, 0.87904344, 0.52320314, 0.8931268 ,\n        0.83943465, 0.8642789 , 0.76474415, 0.58083619, 0.64356226,\n        0.97016158, 0.77084464, 0.86327389, 0.52218137, 0.92909865,\n        0.89834371, 0.84551984, 0.9434564 , 0.91241868, 0.79594526,\n        0.70309897, 0.66166079, 0.62151761, 0.64109359, 0.79558649,\n        0.61770144, 0.73629291, 0.82680175, 0.70028569, 0.60799633]])"},"metadata":{}}]},{"cell_type":"code","source":"def negative_log_loss(y,yhat):\n    m = y.shape[1]\n    return -1/m*np.sum(y*np.log(yhat)+(1-y)*np.log(1-yhat))","metadata":{"execution":{"iopub.status.busy":"2023-07-10T05:34:16.927966Z","iopub.execute_input":"2023-07-10T05:34:16.928463Z","iopub.status.idle":"2023-07-10T05:34:16.935940Z","shell.execute_reply.started":"2023-07-10T05:34:16.928427Z","shell.execute_reply":"2023-07-10T05:34:16.934465Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"loss = negative_log_loss(Y,A3)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T05:34:20.263139Z","iopub.execute_input":"2023-07-10T05:34:20.263582Z","iopub.status.idle":"2023-07-10T05:34:20.269554Z","shell.execute_reply.started":"2023-07-10T05:34:20.263548Z","shell.execute_reply":"2023-07-10T05:34:20.268329Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def sigmoid_backward(x): \n    s = 1/(1+np.exp(-x))\n    return s * (1-s)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T05:34:30.793132Z","iopub.execute_input":"2023-07-10T05:34:30.794510Z","iopub.status.idle":"2023-07-10T05:34:30.800339Z","shell.execute_reply.started":"2023-07-10T05:34:30.794464Z","shell.execute_reply":"2023-07-10T05:34:30.798723Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def relu_backward(x):\n    if x<=0:\n        return 0\n    else:\n        return 1\n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-10T05:53:31.289869Z","iopub.execute_input":"2023-07-10T05:53:31.290348Z","iopub.status.idle":"2023-07-10T05:53:31.296701Z","shell.execute_reply.started":"2023-07-10T05:53:31.290317Z","shell.execute_reply":"2023-07-10T05:53:31.295609Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def linear_forward(A, W, b):\n    \"\"\"\n    Implement the linear part of a layer's forward propagation.\n\n    Arguments:\n    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n\n    Returns:\n    Z -- the input of the activation function, also called pre-activation parameter \n    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    \n    #(≈ 1 line of code)\n    # Z = ...\n    # YOUR CODE STARTS HERE\n    Z=np.dot(W,A)+b\n    \n    # YOUR CODE ENDS HERE\n    cache = (A, W, b)\n    \n    return Z, cache","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def linear_activation_forward(A_prev, W, b, activation):\n    \"\"\"\n    Implement the forward propagation for the LINEAR->ACTIVATION layer\n\n    Arguments:\n    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n\n    Returns:\n    A -- the output of the activation function, also called the post-activation value \n    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n             stored for computing the backward pass efficiently\n    \"\"\"\n    \n    if activation == \"sigmoid\":\n        #(≈ 2 lines of code)\n        # Z, linear_cache = ...\n        # A, activation_cache = ...\n        # YOUR CODE STARTS HERE\n        Z,linear_cache = linear_forward(A_prev, W, b)\n        A,activation_cache = sigmoid(Z)\n        # YOUR CODE ENDS HERE\n    \n    elif activation == \"relu\":\n        #(≈ 2 lines of code)\n        # Z, linear_cache = ...\n        # A, activation_cache = ...\n        # YOUR CODE STARTS HERE\n        Z,linear_cache = linear_forward(A_prev, W, b)\n        A,activation_cache = relu(Z)\n        # YOUR CODE ENDS HERE\n    cache = (linear_cache, activation_cache)\n\n    return A, cache","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def forward_pass(X, parameters):\n\n    caches = []\n    A = X\n    L = len(parameters) // 2                  # number of layers in the neural network\n    \n    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n    # The for loop starts at 1 because layer 0 is the input\n    for l in range(1, L):\n        A_prev = A \n        A, cache = linear_activation_forward(A_prev,parameters['W'+str(l)],parameters['b'+str(l)],'relu')\n        caches.append(cache)\n        # YOUR CODE ENDS HERE\n    \n    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n    #(≈ 2 lines of code)\n    # AL, cache = ...\n    # caches ...\n    # YOUR CODE STARTS HERE\n    AL, cache = linear_activation_forward(A,parameters['W'+str(L)],parameters['b'+str(L)],'sigmoid')\n    caches.append(cache)\n    # YOUR CODE ENDS HERE\n          \n    return AL, caches","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def linear_backward(dZ,cache):\n    A_prev,W,b = cache\n    m = A_prev.shape[1]\n    dW = 1/m*np.dot(dZ,A_prev.T)\n    db = 1/m*np.sum(dZ, axis=1, keepdims=true)\n    dA_prev = np.dot(W.T,dZ)\n    return dA_prev,dW,db","metadata":{"execution":{"iopub.status.busy":"2023-07-10T06:02:59.567732Z","iopub.execute_input":"2023-07-10T06:02:59.568183Z","iopub.status.idle":"2023-07-10T06:02:59.575485Z","shell.execute_reply.started":"2023-07-10T06:02:59.568147Z","shell.execute_reply":"2023-07-10T06:02:59.574328Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def linear_activation_backward(dA, cache, activation):\n  \n    linear_cache, activation_cache = cache\n    Z,cache = activation_cache\n    if activation == \"relu\":\n        #(≈ 2 lines of code)\n        # dZ =  ...\n        # dA_prev, dW, db =  ...\n        # YOUR CODE STARTS HERE\n        dZ = dA*relu_backward(Z)\n        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n        # YOUR CODE ENDS HERE\n        \n    elif activation == \"sigmoid\":\n\n        dZ = dA*sigmoid_backward(Z)\n        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n        \n        # YOUR CODE ENDS HERE\n    \n    return dA_prev, dW, db\n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-10T06:08:30.153112Z","iopub.execute_input":"2023-07-10T06:08:30.153543Z","iopub.status.idle":"2023-07-10T06:08:30.161459Z","shell.execute_reply.started":"2023-07-10T06:08:30.153514Z","shell.execute_reply":"2023-07-10T06:08:30.160005Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def L_model_backward(AL, Y, cache):\n    \"\"\"\n    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n    \n    Arguments:\n    AL -- probability vector, output of the forward propagation (L_model_forward())\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n    \n    Returns:\n    grads -- A dictionary with the gradients\n             grads[\"dA\" + str(l)] = ... \n             grads[\"dW\" + str(l)] = ...\n             grads[\"db\" + str(l)] = ... \n    \"\"\"\n    grads = {}\n    L = len(caches) # the number of layers\n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n    \n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n\n    current_cache = caches[L-1]\n    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL,current_cache,'sigmoid')\n    grads[\"dA\" + str(L-1)] = dA_prev_temp\n    grads[\"dW\" + str(L)] = dW_temp\n    grads[\"db\" + str(L)] = db_temp\n    # YOUR CODE ENDS HERE\n    \n    # Loop from l=L-2 to l=0\n    for l in reversed(range(L-1)):\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)],current_cache,'relu')\n        grads[\"dA\" + str(l)] = dA_prev_temp\n        grads[\"dW\" + str(l+1)] = dW_temp\n        grads[\"db\" + str(l+1)] = db_temp\n    return grads","metadata":{"execution":{"iopub.status.busy":"2023-07-10T06:14:57.186452Z","iopub.execute_input":"2023-07-10T06:14:57.186921Z","iopub.status.idle":"2023-07-10T06:14:57.201377Z","shell.execute_reply.started":"2023-07-10T06:14:57.186889Z","shell.execute_reply":"2023-07-10T06:14:57.199927Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}